# -*- coding: utf-8 -*-
"""vehicle_class.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WsB8_OkAQjk6HI4cb6Z1T4ufenoiYLft

[![Roboflow Notebooks](https://ik.imagekit.io/roboflow/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)

# YOLOv8 Tracking and Counting

---

[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/OS5qI9YBkfk)
[![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/yolov8-tracking-and-counting/)
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/roboflow/notebooks)

Ultralytics YOLOv8 is the latest version of the YOLO (You Only Look Once) object detection and image segmentation model developed by Ultralytics. The YOLOv8 model is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection and image segmentation tasks. It can be trained on large datasets and is capable of running on a variety of hardware platforms, from CPUs to GPUs.

## ‚ö†Ô∏è Disclaimer

YOLOv8 is still under heavy development. Breaking changes are being introduced almost weekly. We strive to make our YOLOv8 notebooks work with the latest version of the library. Last tests took place on **23.01.2023** with version **YOLOv8.0.17**.

If you notice that our notebook behaves incorrectly - especially if you experience errors that prevent you from going through the tutorial - don't hesitate! Let us know and open an [issue](https://github.com/roboflow/notebooks/issues) on the Roboflow Notebooks repository.

## Accompanying Blog Post

We recommend that you follow along in this notebook while reading the blog post on how to train YOLOv8 Tracking and Counting, concurrently.

## Pro Tip: Use GPU Acceleration

If you are running this notebook in Google Colab, navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`. This will ensure your notebook uses a GPU, which will significantly speed up model training times.

## Steps in this Tutorial

In this tutorial, we are going to cover:

- Before you start
- Download video
- Install YOLOv8
- Install ByteTrack
- Install Roboflow Supervision
- Tracking utils
- Load pre-trained YOLOv8 model
- Predict and annotate single frame
- Predict and annotate whole video

**Let's begin!**

## Before you start

Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`.
"""

# !nvidia-smi

# from google.colab import drive
# drive.mount('/content/drive')

import os
# HOME = '/content/drive/MyDrive/Vehicle Classification Videos/'
# print(HOME)

"""## Go to video"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Vehicle Classification Videos/

# video_name = "longer_parking_vid_Trimmed.mp4"
SOURCE_VIDEO_PATH = f"/app/raw_videos/{os.environ['VIDEO_NAME']}"

"""## Install YOLOv8

‚ö†Ô∏è YOLOv8 is still under heavy development. Breaking changes are being introduced almost weekly. We strive to make our YOLOv8 notebooks work with the latest version of the library. Last tests took place on **23.01.2023** with version **YOLOv8.0.17**.

If you notice that our notebook behaves incorrectly - especially if you experience errors that prevent you from going through the tutorial - don't hesitate! Let us know and open an [issue](https://github.com/roboflow/notebooks/issues) on the Roboflow Notebooks repository.
"""

# Pip install method (recommended)

# !pip install ultralytics

from IPython import display
display.clear_output()

import ultralytics
ultralytics.checks()

"""## Install ByteTrack

[ByteTrack](https://github.com/ifzhang/ByteTrack) is great tracker but a bit poorly packaged. We need to jump through some fire hoops to make it work in tandem with [YOLOv8](https://github.com/ultralytics/ultralytics).
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd {HOME}
# !git clone https://github.com/ifzhang/ByteTrack.git
# %cd {HOME}/ByteTrack

# workaround related to https://github.com/roboflow/notebooks/issues/80
# !sed -i 's/onnx==1.8.1/onnx==1.9.0/g' requirements.txt

# !pip3 install -q -r requirements.txt
# !python3 setup.py -q develop
# !pip install -q cython_bbox
# !pip install -q onemetric
# !pip install loguru

from IPython import display
display.clear_output()


import sys
sys.path.append("data/ByteTrack")


import yolox
print("yolox.__version__:", yolox.__version__)

# !pip install lap
from yolox.tracker.byte_tracker import BYTETracker, STrack
from onemetric.cv.utils.iou import box_iou_batch
from dataclasses import dataclass


@dataclass(frozen=True)
class BYTETrackerArgs:
    
    track_thresh: float = 0.40 #was 0.25
    track_buffer: int = 30 #was 30
    match_thresh: float = 0.8 #was 0.8 The threshold for matching detections to tracks in the tracker.
    aspect_ratio_thresh: float = 3.0 #was 3.0 The maximum aspect ratio of a bounding box that will be considered valid.
    min_box_area: float = 6 #was 1.0 The minimum area of a bounding box that will be considered valid.
    mot20: bool = False

"""## Install Roboflow Supervision"""

# !pip install supervision==0.1.0


from IPython import display
display.clear_output()


import supervision
print("supervision.__version__:", supervision.__version__)

from supervision.draw.color import ColorPalette
from supervision.geometry.dataclasses import Point
from supervision.video.dataclasses import VideoInfo
from supervision.video.source import get_video_frames_generator
from supervision.video.sink import VideoSink
from supervision.notebook.utils import show_frame_in_notebook
from supervision.tools.detections import Detections, BoxAnnotator
from supervision.tools.line_counter import LineCounter, LineCounterAnnotator

"""## Tracking utils

Unfortunately, we have to manually match the bounding boxes coming from our model with those created by the tracker.
"""

from typing import List
import numpy as np


# converts Detections into format that can be consumed by match_detections_with_tracks function
def detections2boxes(detections: Detections) -> np.ndarray:
    return np.hstack((
        detections.xyxy,
        detections.confidence[:, np.newaxis]
    ))


# converts List[STrack] into format that can be consumed by match_detections_with_tracks function
def tracks2boxes(tracks: List[STrack]) -> np.ndarray:
    return np.array([
        track.tlbr
        for track
        in tracks
    ], dtype=float)


# matches our bounding boxes with predictions
def match_detections_with_tracks(
    detections: Detections, 
    tracks: List[STrack]
) -> Detections:
    if not np.any(detections.xyxy) or len(tracks) == 0:
        return np.empty((0,))

    tracks_boxes = tracks2boxes(tracks=tracks)
    iou = box_iou_batch(tracks_boxes, detections.xyxy)
    track2detection = np.argmax(iou, axis=1)
    
    tracker_ids = [None] * len(detections)
    
    for tracker_index, detection_index in enumerate(track2detection):
        if iou[tracker_index, detection_index] != 0:
            tracker_ids[detection_index] = tracks[tracker_index].track_id

    return tracker_ids

"""## Load pre-trained YOLOv8 model"""

# settings
MODEL = "yolov8x.pt"

from ultralytics import YOLO

model = YOLO(MODEL)
model.fuse()

"""## Predict and annotate single frame"""

# dict maping class_id to class_name
CLASS_NAMES_DICT = model.model.names
# class_ids of interest - car, motorcycle, bus and truck
CLASS_ID = [2, 3, 5, 7]

# Commented out IPython magic to ensure Python compatibility.
# create frame generator
generator = get_video_frames_generator(SOURCE_VIDEO_PATH)
# create instance of BoxAnnotator
box_annotator = BoxAnnotator(color=ColorPalette(), thickness=4, text_thickness=4, text_scale=2)
# acquire first video frame
iterator = iter(generator)
frame = next(iterator)
# model prediction on single frame and conversion to supervision Detections
results = model(frame)
detections = Detections(
    xyxy=results[0].boxes.xyxy.cpu().numpy(),
    confidence=results[0].boxes.conf.cpu().numpy(),
    class_id=results[0].boxes.cls.cpu().numpy().astype(int)
)
# format custom labels
labels = [
    f"{CLASS_NAMES_DICT[class_id]} {confidence:0.2f}"
    for _, confidence, class_id, tracker_id
    in detections
]
# annotate and display frame
frame = box_annotator.annotate(frame=frame, detections=detections, labels=labels)

# %matplotlib inline
show_frame_in_notebook(frame, (16, 16))

"""## Predict and annotate whole video """

# settings
LINE_START = Point(50, 1500)
LINE_END = Point(3840-50, 1500)

TARGET_VIDEO_PATH = f"/app/processed_videos/{os.environ['VIDEO_NAME']}"

"""Install pymongo"""

# !pip install -q pymongo

# import pymongo
# from pymongo import MongoClient

"""Establish Atlas connection and Collections"""

#client = pymongo.MongoClient("mongodb+srv://testUser:testUser@capstone-482.ibxjskm.mongodb.net/?retryWrites=true&w=majority")

#vehicleDatabase = client.vehicleIdentification

#videoCollection = vehicleDatabase.videoids

#carCollection = vehicleDatabase.vehiclepositions

#changedCollection = vehicleDatabase.changedpositions

"""Insert Video into Video Collection"""

#videoData = {"sourceVideo": SOURCE_VIDEO_PATH, "annotatedVideo": TARGET_VIDEO_PATH, "id": video_name}

#videoCollection.replace_one({"videoId": video_name},videoData,upsert=True)

VideoInfo.from_video_path(SOURCE_VIDEO_PATH)

def seconds_to_minutes_seconds(seconds):
    minutes = int(seconds // 60)
    seconds = int(seconds % 60)
    decimal = seconds%1
    return f"{minutes}:{seconds:02d}.{decimal}"

from numpy.core.numeric import flatnonzero
from numpy.lib import function_base
from tqdm.notebook import tqdm
from pprint import pprint
import math
# create BYTETracker instance
byte_tracker = BYTETracker(BYTETrackerArgs())
# create VideoInfo instance
video_info = VideoInfo.from_video_path(SOURCE_VIDEO_PATH)
# create frame generator
generator = get_video_frames_generator(SOURCE_VIDEO_PATH)
# create LineCounter instance
line_counter = LineCounter(start=LINE_START, end=LINE_END)
# create instance of BoxAnnotator and LineCounterAnnotator
box_annotator = BoxAnnotator(color=ColorPalette(), thickness=2, text_thickness=2, text_scale=1)
line_annotator = LineCounterAnnotator(thickness=2, text_thickness=2, text_scale=1)

# open target video file
with VideoSink(TARGET_VIDEO_PATH, video_info) as sink:
    # loop over video frames
    old_detections = {}
    fps = video_info.fps
    parkCountdown = 2.5 # how many seconds to wait before considering a stopped car as parked
    stopMoveCountdown = .15 # how many seconds to wait before switching states buffer
    movement_count = {}
    countdownP = fps * parkCountdown
    countdownM = fps * stopMoveCountdown

    park = {}
    firstTime = True


    frame_chunk = 2 #how many frames in between each check
    fno = 0 #frame number
    for frame in tqdm(generator, total=video_info.total_frames):
        # model prediction on single frame and conversion to supervision Detections
      
        if (fno)%(frame_chunk)==0:
            results = model(frame)
            detections = Detections(
                xyxy=results[0].boxes.xyxy.cpu().numpy(),
                confidence=results[0].boxes.conf.cpu().numpy(),
                class_id=results[0].boxes.cls.cpu().numpy().astype(int)
            )

            """
            if i==0:
                old_detections = Detections(
                xyxy=results[0].boxes.xyxy.cpu().numpy(),
                confidence=results[0].boxes.conf.cpu().numpy(),
                class_id=results[0].boxes.cls.cpu().numpy().astype(int)
            )
            """
            # filtering out detections with unwanted classes
            mask = np.array([class_id in CLASS_ID for class_id in detections.class_id], dtype=bool)
            detections.filter(mask=mask, inplace=True)
            # tracking detections
            tracks = byte_tracker.update(
                output_results=detections2boxes(detections=detections),
                img_info=frame.shape,
                img_size=frame.shape
            )
            tracker_id = match_detections_with_tracks(detections=detections, tracks=tracks)
            detections.tracker_id = np.array(tracker_id)
            # filtering out detections without trackers
            mask = np.array([tracker_id is not None for tracker_id in detections.tracker_id], dtype=bool)
            detections.filter(mask=mask, inplace=True)


            #tracking detections
            #Converting detections object into detections dictionary sorted by tracker_id (O(N))
            detections_dict = {}
            for _, confidence, class_id, tracker_id in detections:
                info = [_,confidence,class_id]   #["0: Bounding 1: Confidence 2: class_id 3: distance"]
                detections_dict[tracker_id] = info

            #Calculating distance since last frame for each tracker_id
            for tracker_id in detections_dict: 
                if firstTime:
                    park[tracker_id] = 0 
                old_tracker_id = old_detections.get(tracker_id)
                if old_tracker_id!=None:
                    #compare speeds
                    # Calculate the center points of each bounding box
                    new_tracker_id = detections_dict.get(tracker_id)
                    box1_center = [(new_tracker_id[0][0]+new_tracker_id[0][2])/2, (new_tracker_id[0][1]+new_tracker_id[0][3])/2]
                    box2_center = [(old_tracker_id[0][0]+old_tracker_id[0][2])/2, (old_tracker_id[0][1]+old_tracker_id[0][3])/2]

                    # Calculate the distance between the two center points using the Euclidean distance formula
                    distance = math.sqrt((box1_center[0]-box2_center[0])**2 + (box1_center[1]-box2_center[1])**2)
                    # Labels for Moving, Stopped, Parked
                    distance = round(distance, 2)
                    # Your code snippet
                    if distance < 1.5:
                        if tracker_id in park.keys():
                            park[tracker_id] = park[tracker_id] + 1
                        else:
                            park[tracker_id] = 1
                        if park[tracker_id] > countdownP:
                            state = "Parked"
                        else:
                            state = "Stopped"

                        # Reset movement_count for this tracker_id
                        movement_count[tracker_id] = 0
                    else:
                        if tracker_id in movement_count:
                            movement_count[tracker_id] += 1
                        else:
                            movement_count[tracker_id] = 1

                        # Check if the car has been moving for longer than 0.3 seconds
                        if movement_count[tracker_id] > countdownM:  # Assuming countdownM is defined and represents the threshold for 0.3 seconds
                            park[tracker_id] = 0
                            state = "Moving"
                        else:
                            # Maintain the original state when movement doesn't persist for longer than 0.3 seconds
                            if tracker_id in park.keys():
                              if park[tracker_id] > countdownP:
                                state = "Parked"
                              else:
                                state = "Stopped"
                    

                    #detections_dict.get(tracker_id).append(f"{state} {distance}")
                    detections_dict.get(tracker_id).append(state)

                    #Check if state has changed and append True if yes, False if no
                    if(firstTime):
                      detections_dict.get(tracker_id).append("True")
                    elif(detections_dict.get(tracker_id)[3]!=old_detections.get(tracker_id)[3]):
                      detections_dict.get(tracker_id).append("True")
                    else:
                      detections_dict.get(tracker_id).append("False")





                else:
                    detections_dict.get(tracker_id).append(-1)
                    detections_dict.get(tracker_id).append("True")
            
           # print(detections_dict.get(tracker_id))

            # format custom labels
            labels = [
                f"#{detections_dict.get(tracker_id)[3]}"
                for tracker_id in detections_dict
            ]

                #    ["0: Bounding 1: Confidence 2: class_id 3: distance"]

            """
            labels = [
                f"#{tracker_id} {CLASS_NAMES_DICT[class_id]} {confidence:0.2f}"
                for _, confidence, class_id, tracker_id
                in detections
            ]
            """
                        # updating line counter
            line_counter.update(detections=detections)
            # annotate and display frame
            frame = box_annotator.annotate(frame=frame, detections=detections, labels=labels)
            # write label to frame
            line_annotator.annotate(frame=frame, line_counter=line_counter)
            sink.write_frame(frame)
            #Saving current detection as old detections
            old_labels = labels
            old_detections_fr = detections
            old_detections = detections_dict
        else:
            line_counter.update(detections=detections)
            frame = box_annotator.annotate(frame=frame, detections=old_detections_fr, labels=old_labels)
            line_annotator.annotate(frame=frame, line_counter=line_counter)
            sink.write_frame(frame)
        det_list = []

        for tracker_id in detections_dict:
          

          det = {
                 'videoId': os.environ['VIDEO_NAME'],
                 'tracker_id': int(tracker_id), 
                 'class_id':int(detections_dict.get(tracker_id)[2]), 
                 'status':detections_dict.get(tracker_id)[3],
                 'timestamp': round(fno/fps,5), #raw timestamp rounded to 3 decimal places
                 'time': seconds_to_minutes_seconds(fno/fps),
                 'frame': int(fno),
                 'changed': detections_dict.get(tracker_id)[4]
                 }
          #print(det)
          #[print(type(keys)) for keys in det.values()]


    #      if(det.get('changed',"False")=="True"):
       #     changedCollection.insert_one(det)
      #    carCollection.insert_one(det)
      #    det_list.append(det)



        #print(det_list)
        fno+=1
        firstTime = False







"""## üèÜ Congratulations

### Learning Resources

Roboflow has produced many resources that you may find interesting as you advance your knowledge of computer vision:

- [Roboflow Notebooks](https://github.com/roboflow/notebooks): A repository of over 20 notebooks that walk through how to train custom models with a range of model types, from YOLOv7 to SegFormer.
- [Roboflow YouTube](https://www.youtube.com/c/Roboflow): Our library of videos featuring deep dives into the latest in computer vision, detailed tutorials that accompany our notebooks, and more.
- [Roboflow Discuss](https://discuss.roboflow.com/): Have a question about how to do something on Roboflow? Ask your question on our discussion forum.
- [Roboflow Models](https://roboflow.com): Learn about state-of-the-art models and their performance. Find links and tutorials to guide your learning.

### Convert data formats

Roboflow provides free utilities to convert data between dozens of popular computer vision formats. Check out [Roboflow Formats](https://roboflow.com/formats) to find tutorials on how to convert data between formats in a few clicks.

### Connect computer vision to your project logic

[Roboflow Templates](https://roboflow.com/templates) is a public gallery of code snippets that you can use to connect computer vision to your project logic. Code snippets range from sending emails after inference to measuring object distance between detections.
"""